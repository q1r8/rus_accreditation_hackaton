{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Indonesian DistilBERT finetuning with ArcMargin","metadata":{}},{"cell_type":"markdown","source":"In this notebook we are going to first download a DistilBERT model and tokenizer from HuggingFace which is pre-treained on the Indonesian Wikipedia. Then, we fine-tune it on the titles of this dataset with the help of ArcMarginProduct to build more useful embeddings. After that, we can use the model to obtain embeddings for titles in test set and hope that they are representative enough to find similar and dissimilar products.","metadata":{}},{"cell_type":"markdown","source":"If you are not familiar with HuggingFace or BERT models, I've done a tutorial on them on Kaggle and there in addition to explaning how to work with HuggingFace models, I've introduced resources to learn more about NLP and Transformers in general. You can find the notebook [here](https://www.kaggle.com/moeinshariatnia/simple-distilbert-fine-tuning-0-84-lb).","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\nimport math\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-20T19:54:12.149438Z","iopub.execute_input":"2021-11-20T19:54:12.149756Z","iopub.status.idle":"2021-11-20T19:54:14.454822Z","shell.execute_reply.started":"2021-11-20T19:54:12.149726Z","shell.execute_reply":"2021-11-20T19:54:14.454061Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/rosaccred-dataset/result_file_for_training.csv\")\n\ntrain['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'] =\\\n                train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'].apply(lambda x: int(x*100))\n\n# train = train[train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)']\\\n#     .isin(train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'].value_counts().index.tolist()[:50])]\n\n# train = pd.concat([train[train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'] == 930010].sample(4000),\\\n#                   train[~(train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'] == 930010)]])\n# train = train.sample(train.shape[0]).reset_index(drop=True)\n\ndisplay(train.head(), train.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:14.456468Z","iopub.execute_input":"2021-11-20T19:54:14.456782Z","iopub.status.idle":"2021-11-20T19:54:17.560982Z","shell.execute_reply.started":"2021-11-20T19:54:14.456748Z","shell.execute_reply":"2021-11-20T19:54:17.560158Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The following histogram gives us an idea that roughly how many words are there in each title. It is not a precise count of the tokens fed to the model because DistilBERT tokenizer does a more sophisticated function than simply splitting the sentence from its white spaces.","metadata":{}},{"cell_type":"code","source":"title_lengths = train['Общее наименование продукции'].apply(lambda x: len(x.split(\" \"))).to_numpy()\nprint(f\"MIN words: {title_lengths.min()}, MAX words: {title_lengths.max()}\")\nplt.hist(title_lengths);","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:06:32.751288Z","iopub.execute_input":"2021-11-20T11:06:32.751542Z","iopub.status.idle":"2021-11-20T11:06:33.088031Z","shell.execute_reply.started":"2021-11-20T11:06:32.751517Z","shell.execute_reply":"2021-11-20T11:06:33.087166Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"max_length is set to 30 according to the histogram. But you can safely change it.","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:43:31.810528Z","iopub.execute_input":"2021-11-20T15:43:31.810869Z","iopub.status.idle":"2021-11-20T15:43:31.829881Z","shell.execute_reply.started":"2021-11-20T15:43:31.810839Z","shell.execute_reply":"2021-11-20T15:43:31.829033Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    DistilBERT = True # if set to False, BERT model will be used\n    bert_hidden_size = 768\n    \n    batch_size = 64\n    epochs = 30\n    num_workers = 4\n    learning_rate = 1e-5 #3e-5\n    scheduler = \"ReduceLROnPlateau\"\n    step = 'epoch'\n    patience = 2\n    factor = 0.8\n    dropout = 0.5\n    model_path = \"/kaggle/working\"\n    max_length = 30\n    model_save_name = \"model.pt\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:18.165684Z","iopub.execute_input":"2021-11-20T19:54:18.166014Z","iopub.status.idle":"2021-11-20T19:54:18.236039Z","shell.execute_reply.started":"2021-11-20T19:54:18.165976Z","shell.execute_reply":"2021-11-20T19:54:18.235200Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Loading the model and its tokenizer from amazing HuggingFace model hub. As mentioned before, this model has been pre-trained on indonesian wikipedia.","metadata":{}},{"cell_type":"code","source":"if CFG.DistilBERT:\n    model_name='cahya/distilbert-base-indonesian'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    bert_model = DistilBertModel.from_pretrained(model_name)\nelse:\n    model_name='cahya/bert-base-indonesian-522M'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    bert_model = BertModel.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:30.927718Z","iopub.execute_input":"2021-11-20T19:54:30.928073Z","iopub.status.idle":"2021-11-20T19:54:47.322785Z","shell.execute_reply.started":"2021-11-20T19:54:30.928039Z","shell.execute_reply":"2021-11-20T19:54:47.322004Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"See an example","metadata":{}},{"cell_type":"code","source":"text = train['title'].values[np.random.randint(0, len(train) - 1, 1)[0]]\nprint(f\"Text of the title: {text}\")\nencoded_input = tokenizer(text, return_tensors='pt')\nprint(f\"Input tokens: {encoded_input['input_ids']}\")\ndecoded_input = tokenizer.decode(encoded_input['input_ids'][0])\nprint(f\"Decoded tokens: {decoded_input}\")\noutput = bert_model(**encoded_input)\nprint(f\"last layer's output shape: {output.last_hidden_state.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:28.431987Z","iopub.status.idle":"2021-11-20T19:54:28.432695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"markdown","source":"Encoding label_group coulmn to numeric labels so we can feed them to the model and loss function.","metadata":{}},{"cell_type":"code","source":"lbl_encoder = LabelEncoder()\ntrain['label_code'] = lbl_encoder.fit_transform(train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'])\nNUM_CLASSES = train['label_code'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.324558Z","iopub.execute_input":"2021-11-20T19:54:47.324895Z","iopub.status.idle":"2021-11-20T19:54:47.350370Z","shell.execute_reply.started":"2021-11-20T19:54:47.324861Z","shell.execute_reply":"2021-11-20T19:54:47.349707Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.351711Z","iopub.execute_input":"2021-11-20T19:54:47.352061Z","iopub.status.idle":"2021-11-20T19:54:47.358428Z","shell.execute_reply.started":"2021-11-20T19:54:47.352024Z","shell.execute_reply":"2021-11-20T19:54:47.357329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            self.targets = dataframe['label_code'].values\n        texts = list(dataframe['Общее наименование продукции'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        \n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).long()\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.360173Z","iopub.execute_input":"2021-11-20T19:54:47.360693Z","iopub.status.idle":"2021-11-20T19:54:47.370356Z","shell.execute_reply.started":"2021-11-20T19:54:47.360650Z","shell.execute_reply":"2021-11-20T19:54:47.369438Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = TextDataset(train.sample(100), tokenizer, max_length=CFG.max_length)\ndataloader = torch.utils.data.DataLoader(dataset, \n#                                          batch_size=CFG.batch_size, \n                                         num_workers=CFG.num_workers, \n                                         shuffle=True)\nbatch = next(iter(dataloader))\nprint(batch['input_ids'].shape, batch['labels'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.373406Z","iopub.execute_input":"2021-11-20T19:54:47.374247Z","iopub.status.idle":"2021-11-20T19:54:47.649789Z","shell.execute_reply.started":"2021-11-20T19:54:47.374210Z","shell.execute_reply":"2021-11-20T19:54:47.648767Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.651457Z","iopub.execute_input":"2021-11-20T19:54:47.651745Z","iopub.status.idle":"2021-11-20T19:54:47.658337Z","shell.execute_reply.started":"2021-11-20T19:54:47.651718Z","shell.execute_reply":"2021-11-20T19:54:47.657406Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# code from https://github.com/ronghuaiyang/arcface-pytorch/blob/47ace80b128042cd8d2efd408f55c5a3e156b032/models/metrics.py#L10\n\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.659758Z","iopub.execute_input":"2021-11-20T19:54:47.660465Z","iopub.status.idle":"2021-11-20T19:54:47.673303Z","shell.execute_reply.started":"2021-11-20T19:54:47.660425Z","shell.execute_reply":"2021-11-20T19:54:47.672326Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, \n                 bert_model, \n                 num_classes=NUM_CLASSES, \n                 last_hidden_size=CFG.bert_hidden_size):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        self.arc_margin = ArcMarginProduct(last_hidden_size, \n                                           num_classes,\n                                           s=30.0, \n                                           m=0.50, \n                                           easy_margin=False)\n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n    \n    def forward(self, batch):\n        CLS_hidden_state = self.get_bert_features(batch)\n#         output = self.arc_margin(CLS_hidden_state, batch['labels'])\n        return CLS_hidden_state","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.674882Z","iopub.execute_input":"2021-11-20T19:54:47.675452Z","iopub.status.idle":"2021-11-20T19:54:47.685816Z","shell.execute_reply.started":"2021-11-20T19:54:47.675416Z","shell.execute_reply":"2021-11-20T19:54:47.685057Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n    \n    def reset(self):\n        self.avg, self.sum, self.count = [0]*3\n    \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n    \n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef one_epoch(model, \n              criterion, \n              loader,\n              optimizer=None, \n              lr_scheduler=None, \n              mode=\"train\", \n              step=\"batch\"):\n    \n    loss_meter = AvgMeter()\n    acc_meter = AvgMeter()\n    \n    tqdm_object = tqdm(loader, total=len(loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        preds = model(batch)\n        loss = criterion(preds, batch['labels'])\n        if mode == \"train\":\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if step == \"batch\":\n                lr_scheduler.step()\n                \n        count = batch['input_ids'].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        accuracy = get_accuracy(preds.detach(), batch['labels'])\n        acc_meter.update(accuracy.item(), count)\n        if mode == \"train\":\n            tqdm_object.set_postfix(train_loss=loss_meter.avg, accuracy=acc_meter.avg, lr=get_lr(optimizer))\n        else:\n            tqdm_object.set_postfix(valid_loss=loss_meter.avg, accuracy=acc_meter.avg)\n    \n    return loss_meter, acc_meter\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]\n\ndef get_accuracy(preds, targets):\n    \"\"\"\n    preds shape: (batch_size, num_labels)\n    targets shape: (batch_size)\n    \"\"\"\n    preds = preds.argmax(dim=1)\n    acc = (preds == targets).float().mean()\n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.687348Z","iopub.execute_input":"2021-11-20T19:54:47.687750Z","iopub.status.idle":"2021-11-20T19:54:47.701997Z","shell.execute_reply.started":"2021-11-20T19:54:47.687714Z","shell.execute_reply":"2021-11-20T19:54:47.701266Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train_eval(epochs, model, train_loader, valid_loader, \n               criterion, optimizer, lr_scheduler=None):\n    \n    best_loss = float('inf')\n    best_model_weights = copy.deepcopy(model.state_dict())\n    \n    for epoch in range(epochs):\n        print(\"*\" * 30)\n        print(f\"Epoch {epoch + 1}\")\n        current_lr = get_lr(optimizer)\n        \n        model.train()\n        train_loss, train_acc = one_epoch(model, \n                                          criterion, \n                                          train_loader, \n                                          optimizer=optimizer,\n                                          lr_scheduler=lr_scheduler,\n                                          mode=\"train\",\n                                          step=CFG.step)                     \n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_acc = one_epoch(model, \n                                              criterion, \n                                              valid_loader, \n                                              optimizer=None,\n                                              lr_scheduler=None,\n                                              mode=\"valid\")\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            best_model_weights = copy.deepcopy(model.state_dict())\n            tmp_model_state = model.state_dict()\n            torch.save(model.state_dict(), f'{CFG.model_path}/{CFG.model_save_name}')\n            print(\"Saved best model!\")\n            break\n        \n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            lr_scheduler.step(valid_loss.avg)\n            if current_lr != get_lr(optimizer):\n                print(\"Loading best model weights!\")\n                model.load_state_dict(torch.load(f'{CFG.model_path}/{CFG.model_save_name}', \n                                                 map_location=CFG.device))\n        \n        print(\"*\" * 30)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:54:47.704554Z","iopub.execute_input":"2021-11-20T19:54:47.704785Z","iopub.status.idle":"2021-11-20T19:54:47.716591Z","shell.execute_reply.started":"2021-11-20T19:54:47.704763Z","shell.execute_reply":"2021-11-20T19:54:47.715909Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(train, \n                                      test_size=0.33, \n                                      shuffle=True, \n                                      random_state=42,)\n#                                       stratify=train['label_code'])\n\ntrain_dataset = TextDataset(train_df, tokenizer, max_length=CFG.max_length)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=True)\n\nvalid_dataset = TextDataset(valid_df, tokenizer, max_length=CFG.max_length)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:59:40.807839Z","iopub.execute_input":"2021-11-20T19:59:40.808187Z","iopub.status.idle":"2021-11-20T20:06:21.436533Z","shell.execute_reply.started":"2021-11-20T19:59:40.808157Z","shell.execute_reply":"2021-11-20T20:06:21.435735Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = Model(bert_model).to(CFG.device)\nmodel.state_dict()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\nif CFG.scheduler == \"ReduceLROnPlateau\":\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                              mode=\"min\", \n                                                              factor=CFG.factor, \n                                                              patience=CFG.patience)\n\ntrain_eval(CFG.epochs, model, train_loader, valid_loader,\n           criterion, optimizer, lr_scheduler=lr_scheduler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir tokenizer\n# tokenizer.save_pretrained(\"./tokenizer\")\ntorch.save(model.state_dict(), \"final.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(bert_model)\nmodel.load_state_dict(torch.load('../input/zaebalomenya-eto-vse/model(1).pt', map_location=torch.device('cpu')))\nmodel.cpu()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:47:48.634312Z","iopub.execute_input":"2021-11-20T12:47:48.634847Z","iopub.status.idle":"2021-11-20T12:47:49.311222Z","shell.execute_reply.started":"2021-11-20T12:47:48.634790Z","shell.execute_reply":"2021-11-20T12:47:49.309940Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_predicts(model, dataloader):\n    tqdm_object = tqdm(dataloader, total=len(dataloader))\n    preds = []\n    for batch in tqdm_object:\n        batch = {k: v.cuda() for k, v in batch.items()}\n        preds.append(model(batch))\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:55:56.702533Z","iopub.execute_input":"2021-11-20T19:55:56.702864Z","iopub.status.idle":"2021-11-20T19:55:56.708307Z","shell.execute_reply.started":"2021-11-20T19:55:56.702834Z","shell.execute_reply":"2021-11-20T19:55:56.707445Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = Model(bert_model)\nmodel.load_state_dict(torch.load('../input/zaebalomenya-eto-vse/model(1).pt'))\nmodel.eval()\nmodel.cuda();\n\ncatogories = train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'].unique()\nbase_vectors_for_unique_categories = {}\nfor subCategory in tqdm(catogories):\n    dataframe_categories = train[train['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)']== subCategory]\n    if dataframe_categories.shape[0] > 500:\n        corpus = dataframe_categories.sample(500).reset_index(drop=True)\n    else:\n        corpus = dataframe_categories.reset_index(drop=True)\n        \n    dataset = TextDataset(corpus, tokenizer, max_length=CFG.max_length)\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             batch_size=32,\n                                             num_workers=CFG.num_workers, \n                                             shuffle=True)\n    embedings = get_predicts(model, dataloader)\n    \n    \n    mean_emb = np.zeros(embedings[0].shape[1])\n    for predict in embedings:\n        mean_emb += predict.mean(axis=0).cpu().detach().numpy()\n    mean_emb /= len(embedings)\n    \n                         \n    base_vectors_for_unique_categories.update({str(subCategory):mean_emb})\n{'base_vectors':base_vectors_for_unique_categories}","metadata":{"execution":{"iopub.status.busy":"2021-11-20T19:55:57.861254Z","iopub.execute_input":"2021-11-20T19:55:57.861573Z","iopub.status.idle":"2021-11-20T19:56:07.354450Z","shell.execute_reply.started":"2021-11-20T19:55:57.861543Z","shell.execute_reply":"2021-11-20T19:56:07.352854Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"json_with_emb = pd.DataFrame(base_vectors_for_unique_categories)\njson_with_emb.to_csv('df_with_embs.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T13:31:06.581393Z","iopub.execute_input":"2021-11-20T13:31:06.581785Z","iopub.status.idle":"2021-11-20T13:31:06.716387Z","shell.execute_reply.started":"2021-11-20T13:31:06.581752Z","shell.execute_reply":"2021-11-20T13:31:06.715121Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"json_with_emb = pd.read_csv('../input/category-embs/df_with_embs.csv')\ntest_df = train.sample(50).reset_index(drop=True)\n# test_df = pd.DataFrame([{'Общее наименование продукции':'Термос-бутылка с маркировкой \"Naked\",\\\n#             предназначенная для использования с пищевой продукцией'}])\ntest_df['predict'] = None\nfor ind in range(test_df.shape[0]-1):\n    dataset = TextDataset(test_df.iloc[ind:ind+1, :], tokenizer, max_length=CFG.max_length)\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                         batch_size=1,\n                                         num_workers=CFG.num_workers, \n                                         shuffle=True)\n    predicts = get_predicts(model, dataloader)[0][0].cpu().detach().numpy()\n\n    dists = np.sum((np.square(predicts - json_with_emb.values.T)), axis=1)\n    indices = np.argsort(dists)\n    predict_category = json_with_emb.columns[indices[0]]\n    test_df.loc[ind, 'predict'] = predict_category\n    indexes = test_df[test_df['predict'] != test_df\\\n                      ['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)']].index\n    test_df.iloc[indexes]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:50:27.967841Z","iopub.execute_input":"2021-11-20T16:50:27.968207Z","iopub.status.idle":"2021-11-20T16:50:38.656723Z","shell.execute_reply.started":"2021-11-20T16:50:27.968172Z","shell.execute_reply":"2021-11-20T16:50:38.655705Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ntest_df = test_df[~(test_df['predict'].isna())]\ntest_df['predict'] = test_df['predict'].apply(lambda x: int(x))\naccuracy = accuracy_score(test_df['predict'].values,\\\n               test_df['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'].values)\nprint(f'На рандомной выборке получили точность модели равную {round(accuracy*100, 2)}')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:50:38.658819Z","iopub.execute_input":"2021-11-20T16:50:38.659207Z","iopub.status.idle":"2021-11-20T16:50:38.672396Z","shell.execute_reply.started":"2021-11-20T16:50:38.659177Z","shell.execute_reply":"2021-11-20T16:50:38.670981Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Для ЦПУ без лейбла","metadata":{}},{"cell_type":"code","source":"model = Model(bert_model)\nmodel.load_state_dict(torch.load('../input/zaebalomenya-eto-vse/model(1).pt', map_location=torch.device('cpu')))\nmodel.cpu();","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:50:42.557416Z","iopub.execute_input":"2021-11-20T16:50:42.557751Z","iopub.status.idle":"2021-11-20T16:50:42.920860Z","shell.execute_reply.started":"2021-11-20T16:50:42.557716Z","shell.execute_reply":"2021-11-20T16:50:42.919991Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"json_with_emb","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:53:41.572477Z","iopub.execute_input":"2021-11-20T16:53:41.572853Z","iopub.status.idle":"2021-11-20T16:53:41.605817Z","shell.execute_reply.started":"2021-11-20T16:53:41.572819Z","shell.execute_reply":"2021-11-20T16:53:41.604970Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"test_df = train.sample(50).reset_index(drop=True)\ntest_df['predict'] = None\nfor cpu_index in range(test_df.shape[0]+1):\n    dataset = TextDataset(test_df.iloc[cpu_index:cpu_index+1, :], tokenizer, max_length=CFG.max_length, mode='test')\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             batch_size=1,\n                                             num_workers=CFG.num_workers, \n                                             shuffle=True)\n    tqdm_object = tqdm(dataloader, total=len(dataloader))\n    preds = []\n    model.cpu()\n    model.eval()\n    for batch in tqdm_object:\n        with torch.no_grad():\n            batch = {k: v.cpu() for k, v in batch.items()}\n            preds.append(model(batch))\n\n    dists = np.sum((np.square(preds[0][0].cpu().detach().numpy() - json_with_emb.values.T)), axis=1)\n    indices = np.argsort(dists)\n    predict_category = json_with_emb.columns[indices[0]]\n    test_df.loc[cpu_index, 'predict'] = predict_category\npredict_category","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:59:57.921100Z","iopub.execute_input":"2021-11-20T16:59:57.921480Z","iopub.status.idle":"2021-11-20T17:00:13.128648Z","shell.execute_reply.started":"2021-11-20T16:59:57.921447Z","shell.execute_reply":"2021-11-20T17:00:13.126680Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:00:33.073391Z","iopub.execute_input":"2021-11-20T17:00:33.073737Z","iopub.status.idle":"2021-11-20T17:00:33.097687Z","shell.execute_reply.started":"2021-11-20T17:00:33.073706Z","shell.execute_reply":"2021-11-20T17:00:33.096797Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"test_df.iloc[41, 1]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:25:12.768530Z","iopub.execute_input":"2021-11-20T17:25:12.768900Z","iopub.status.idle":"2021-11-20T17:25:12.774980Z","shell.execute_reply.started":"2021-11-20T17:25:12.768851Z","shell.execute_reply":"2021-11-20T17:25:12.773991Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"test_df = test_df[~(test_df['predict'].isna())]\ntest_df['predict'] = test_df['predict'].apply(lambda x: int(x))\n\naccuracy_score(test_df['predict'].values,\\\n               test_df['Раздел ЕП РФ (Код из ФГИС ФСА для подкатегории продукции)'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T17:00:46.970100Z","iopub.execute_input":"2021-11-20T17:00:46.970445Z","iopub.status.idle":"2021-11-20T17:00:46.980264Z","shell.execute_reply.started":"2021-11-20T17:00:46.970413Z","shell.execute_reply":"2021-11-20T17:00:46.979328Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# predict with tensor and label","metadata":{}},{"cell_type":"code","source":"dists = np.sum((np.square(predicts - np.array(list(base_vectors_for_unique_categories.values())))), axis=1)\nindices = np.argsort(dists)[:5]\npredict_category = list(base_vectors_for_unique_categories.keys())[indices[0]]\npredict_category","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:44:23.617544Z","iopub.execute_input":"2021-11-20T11:44:23.617892Z","iopub.status.idle":"2021-11-20T11:44:23.627114Z","shell.execute_reply.started":"2021-11-20T11:44:23.617859Z","shell.execute_reply":"2021-11-20T11:44:23.625996Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"indices = np.argsort(dists)[:5]\nlist(base_vectors_for_unique_categories.keys())[indices[0]]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:40:58.182395Z","iopub.execute_input":"2021-11-20T11:40:58.182890Z","iopub.status.idle":"2021-11-20T11:40:58.188573Z","shell.execute_reply.started":"2021-11-20T11:40:58.182853Z","shell.execute_reply":"2021-11-20T11:40:58.187398Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"list(base_vectors_for_unique_categories.keys())[10]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:42:51.106599Z","iopub.execute_input":"2021-11-20T11:42:51.106930Z","iopub.status.idle":"2021-11-20T11:42:51.112637Z","shell.execute_reply.started":"2021-11-20T11:42:51.106882Z","shell.execute_reply":"2021-11-20T11:42:51.111625Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"dataset = TextDataset(train.sample(500), tokenizer, max_length=CFG.max_length)\ndataloader = torch.utils.data.DataLoader(dataset,\n                                         batch_size=64,\n                                         num_workers=CFG.num_workers, \n                                         shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:04:28.977357Z","iopub.execute_input":"2021-11-20T11:04:28.977682Z","iopub.status.idle":"2021-11-20T11:04:29.653144Z","shell.execute_reply.started":"2021-11-20T11:04:28.977650Z","shell.execute_reply":"2021-11-20T11:04:29.652239Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predicts(model, dataloader):\n    tqdm_object = tqdm(dataloader, total=len(dataloader))\n    preds = []\n    for batch in tqdm_object:\n        batch = {k: v.cuda() for k, v in batch.items()}\n        preds.append(model(batch))\n    return preds\n\npredicts = get_predicts(model, dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:11:37.885118Z","iopub.execute_input":"2021-11-20T11:11:37.885517Z","iopub.status.idle":"2021-11-20T11:11:38.142112Z","shell.execute_reply.started":"2021-11-20T11:11:37.885480Z","shell.execute_reply":"2021-11-20T11:11:38.141038Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"predicts[0][0]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:11:53.336871Z","iopub.execute_input":"2021-11-20T11:11:53.337254Z","iopub.status.idle":"2021-11-20T11:11:53.365821Z","shell.execute_reply.started":"2021-11-20T11:11:53.337223Z","shell.execute_reply":"2021-11-20T11:11:53.364996Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"mean_embs = np.zeros(predicts[0].shape[1])\nfor predict in predicts:\n    mean_embs += predict.mean(axis=0).cpu().detach().numpy()\nmean_embs.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:03:08.105396Z","iopub.execute_input":"2021-11-20T11:03:08.105721Z","iopub.status.idle":"2021-11-20T11:03:08.113706Z","shell.execute_reply.started":"2021-11-20T11:03:08.105689Z","shell.execute_reply":"2021-11-20T11:03:08.112760Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"predicts[0].shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:01:25.871586Z","iopub.execute_input":"2021-11-20T11:01:25.871927Z","iopub.status.idle":"2021-11-20T11:01:25.880748Z","shell.execute_reply.started":"2021-11-20T11:01:25.871894Z","shell.execute_reply":"2021-11-20T11:01:25.879801Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"np.zeros(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:01:03.711581Z","iopub.execute_input":"2021-11-20T11:01:03.711939Z","iopub.status.idle":"2021-11-20T11:01:03.719192Z","shell.execute_reply.started":"2021-11-20T11:01:03.711905Z","shell.execute_reply":"2021-11-20T11:01:03.717989Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"len(predicts)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:00:01.894811Z","iopub.execute_input":"2021-11-20T11:00:01.895178Z","iopub.status.idle":"2021-11-20T11:00:01.901336Z","shell.execute_reply.started":"2021-11-20T11:00:01.895148Z","shell.execute_reply":"2021-11-20T11:00:01.900114Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"mean_embs[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T10:59:14.579366Z","iopub.execute_input":"2021-11-20T10:59:14.579703Z","iopub.status.idle":"2021-11-20T10:59:14.606960Z","shell.execute_reply.started":"2021-11-20T10:59:14.579666Z","shell.execute_reply":"2021-11-20T10:59:14.606051Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"text = 'max_length=maxl, pad_to_max_length=True, truncation=True'\nfrom torch.utils.data import TensorDataset, DataLoader\nX_test = torch.tensor(tokenizer.encode(text, max_length=30, pad_to_max_length=True, truncation=True))\ntest_data = TensorDataset(X_test)\ntest_dataloader = DataLoader(\n    test_data,\n    batch_size=1,\n    num_workers=4,\n    pin_memory=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_dataloader:\n    batch = batch[0]\n    batch.cuda()\n    with torch.no_grad():\n        logits = model(batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch['input_ids'][0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[0, 'Общее наименование продукции']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_vector(text, tokenizer_model):\n    token = tokenizer_model.encode(text, max_length=30, pad_to_max_length=True, truncation=True)\n    token_with_dop_dimension = np.expand_dims(token, axis=0)\n    return torch.tensor(token_with_dop_dimension).to('cuda')\n\nmodel(model_vector(train.loc[0, 'Общее наименование продукции'], tokenizer))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel(batch['input_ids'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_['bert_model.embeddings.word_embeddings.weight'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ = torch.load('../input/zaebalomenya-eto-vse/model(1).pt')\nmodel_['arc_margin.weight'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}